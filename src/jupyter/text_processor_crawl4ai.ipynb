{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets pull up the scraped file from crawl4ai as in that markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "current_root = os.getcwd()\n",
    "logger.info(f\"current_root: {current_root}\")\n",
    "\n",
    "markdown_text = open(\"../../data/raw_content/web_scrape_result_2025_01_03-04_58_17.md\", 'r', encoding='utf-8').read()\n",
    "\n",
    "print(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can detect the title of the paper\n",
    "\n",
    "@Not actually used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alzheimer's disease stage recognition from MRI and PET imaging data using Pareto-optimal quantum dynamic optimization\n"
     ]
    }
   ],
   "source": [
    "title_match = re.search(r'^#\\s+(.+)$', markdown_text, re.MULTILINE)\n",
    "if title_match:\n",
    "    print(title_match.group(1).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(content):\n",
    "  # Remove reference numbers and links\n",
    "  content = re.sub(r'\\[\\d+\\]', '', content)\n",
    "\n",
    "  # Format whitespace\n",
    "  content = re.sub(r'\\s+', ' ', content)\n",
    "\n",
    "  # Remove backslash\n",
    "  content = re.sub(r'\\\\', '', content)\n",
    "\n",
    "  # Remove html tags\n",
    "  content = re.sub(r'<[^>]*>', '', content)\n",
    "\n",
    "  # Remove newlines\n",
    "  content = re.sub(r'\\n', '', content)\n",
    "\n",
    "  # Remove whitespace at the beginning and end of the string\n",
    "  content = content.strip()\n",
    "  return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^(#{1,4})\\s+(.+?)\\n((?:(?!^#{1,4}\\s+).|\\n)*)'\n",
    "matches = re.finditer(pattern, markdown_text, re.MULTILINE) # prevent overstacking\n",
    "\n",
    "extracted_content = {}\n",
    "\n",
    "for match in matches:\n",
    "  title = match.group(2).strip()\n",
    "  content = match.group(3).strip()\n",
    "\n",
    "  if title and content:\n",
    "    extracted_content[clean_content(title)] = clean_content(content)\n",
    "\n",
    "print(extracted_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For demonstration, find abstract, discussion and conclusion\n",
    "\n",
    "Actually,its in json format so that I can easily access the value, but for demonstration, I will just use a markdown file which is more readable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Abstract\n",
      "The threat posed by Alzheimer's disease (AD) to human health has grown significantly. However, the precise diagnosis and classification of AD stages remain a challenge. Neuroimaging methods such as structural magnetic resonance imaging (sMRI) and fluorodeoxyglucose positron emission tomography (FDG-PET) have been used to diagnose and categorize AD. However, feature selection approaches that are frequently used to extract additional data from multimodal imaging are prone to errors. This paper suggests using a static pulse-coupled neural network and a Laplacian pyramid to combine sMRI and FDG-PET data. After that, the fused images are used to train the Mobile Vision Transformer (MViT), optimized with Pareto-Optimal Quantum Dynamic Optimization for Neural Architecture Search, while the fused images are augmented to avoid overfitting and then classify unfused MRI and FDG-PET images obtained from the AD Neuroimaging Initiative (ADNI) and Open Access Series of Imaging Studies (OASIS) datasets into various stages of AD. The architectural hyperparameters of MViT are optimized using Quantum Dynamic Optimization, which ensures a Pareto-optimal solution. The Peak Signal-to-Noise Ratio (PSNR), the Mean Squared Error (MSE), and the Structured Similarity Indexing Method (SSIM) are used to measure the quality of the fused image. We found that the fused image was consistent in all metrics, having 0.64 SIMM, 35.60 PSNR, and 0.21 MSE for the FDG-PET image. In the classification of AD vs. cognitive normal (CN), AD vs. mild cognitive impairment (MCI), and CN vs. MCI, the precision of the proposed method is 94.73%, 92.98% and 89.36%, respectively. The sensitivity is 90. 70%, 90. 70%, and 90. 91% while the specificity is 100%, 100%, and 85. 71%, respectively, in the ADNI MRI test data. **Keywords:** Alzheimer's disease, Pareto optimization, Deep learning, Classification, Image fusion, Multimodal\n",
      "\n",
      "## Discussion\n",
      "From a medical point of view, the use of AI algorithms for the evaluation of fused MRI and PET images presents several distinct advantages over manual evaluations conducted by specialists, particularly in certain contexts. To begin with, the manual evaluation of MRI and PET images is a labor-intensive process that requires specialized training and is susceptible to high measurement variability arising from diverse protocols [](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/). In contrast, AI has a remarkable ability to quickly process large volumes of medical imaging data, outpacing human experts in efficiency. This is particularly noteworthy in conditions such as Alzheimer's, where timely detection is of paramount importance. AI's proficiency in scrutinizing intricate patterns within fused MRI and PET scans contributes significantly to the early diagnosis of the disease. AI's forte in pattern recognition empowers it to discern subtle changes or patterns in medical images that may pose challenges for human experts. In addition, algorithms are adept at seamlessly integrating and analyzing diverse datasets, ensuring a high level of consistency in evaluations. This attribute facilitates a more comprehensive assessment of the patient's condition [](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/). This consistency is of particular significance in the longitudinal monitoring of disease progression over time. Current diagnostic approaches for AD begin to focus primarily on modern imaging modalities, with a distinct focus on detecting either amyloid deposition or neurodegeneration [](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/). The study's findings affirm the capacity of AI to make meaningful contributions to early diagnosis, demonstrating its accuracy in distinguishing between various stages of the disease. This reinforces the notion that AI, with its efficiency, pattern recognition capabilities, and consistent evaluations, holds significant promise in enhancing medical diagnostics and patient care. The results (see [Table 3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/)) indicate that a highly effective multiscale fusion technique can be achieved when images are fused at the sixth level of the LP. This finding is significant, as it suggests that image fusion can be optimized by selecting the appropriate level of the LP. The fused images produced using this technique were found to be sufficiently detailed replicas of the original images, as confirmed by the SSIM readings at level 6 of the LP. Specifically, 0.64 for AD on PET images and 0.60 for AD on MRI images. The average PSNR was approximately 34.80 dB, and the average MSE was approximately 0.213. The quality of the fusion method suggested that our method maintained the spatial information of the magnetic resonance image and the spectral characteristics of the PET images [](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/), and this provides further evidence that our proposed method achieved a fused image of good quality. After training different variants of ViT (Simple and Deep) and Mobile ViT on our fused data, we discovered that the Pareto-optimized approach of MViT achieved higher accuracy than the other two ViT variants. As shown in [Table 5](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/), our proposed method, which uses Pareto optimization on a mobile ViT and MRI / PET fusion data, performed better than traditional image fusion techniques and deep learning algorithms used with the two ViT variants were selected. The ablation study result depicted in [Table 8](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/) shows that there are no significant changes in training accuracy across different variations of the fused image (with or without sharpening and SPCNN) when compared to [Table 5](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/), and this suggests that the model might be effectively learning from the training data regardless of the specific preprocessing techniques applied to the input images. The consistency in training accuracy across different variations of the fused image shows that the model is capable of learning from the provided training data regardless of whether sharpening or SPCNN is applied. This could indicate that other factors, such as the inherent quality and informativeness of the training data, are more influential in determining training accuracy. [Table 9](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/), [Table 10](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/), and [Table 11](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/) depicts the ablation study results when the model is tested with test data, the model trained with Fused image without sharpening and SPCNN, Fused image without sharpening but with SPCNN, and Fused image without SPCNN but with sharpening respectively. The model trained with fused image with sharpening and SPCNN generalized well as shown in [Table 6](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/) with 9.73% increament in accuracy for AD vs CN on ADNI (MRI). Although the impact of SPCNN is not so significant as seen in [Table 10](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/), in which 0.18% increment was achieved in AD vs CN on ADNI (MRI) test data. However, the fact that the model trained with the fused image with sharpening and SPCNN generalized well on the test data indicates that these preprocessing techniques help to improve the model's ability to perform well on unseen data. Sharpening enhances the structural details in the images, while SPCNN fusion integrates both structural and functional information effectively, leading to a more informative representation for the model to learn from. Overfitting was able to be controlled by augmenting the data and with the use of weight decay regularization during optimization, which encourages the weights of the model to be smaller. GLP and SPCNN provide several benefits compared to the DWT. To begin with, both methods excel at maintaining image details and reducing noise, leading to superior image representations. Additionally, SPCNN exhibits greater resilience to image distortions and fluctuations compared to DWT, making it a more efficient option for practical image processing tasks. The combination of SPCNN and GLP has been shown to deliver better results with respect to low feature dimensionality, as DWT breaks images into sub-bands of predetermined bandwidths, while GLP allows greater flexibility in selecting the bandwidth for each subband [](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/). The GLP-SPCNN combination outperforms DWT and other approaches, offering advantages like maintaining image details, reducing noise, and being more resistant to distortions. GLP-enabled mobile ViT achieved 100% specificity in identifying CN and MCI without mis-classification. It surpasses other ViT variants and image fusion techniques in AD classification accuracy, addressing overfitting through data augmentation, regularization (dropout), epochs and learning rate scheduler. [Fig. 5](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/) shows that the utilization of 0.3 dropout, learning rate scheduler (mode = min, patience = 2), and epochs of 100) controlled the overfitting of the model. The proposed model enables better spatial connections between AD class features, leading to precise AD stage classification. It achieved a low false positive rate, ensuring a reliable identification of individuals without AD, which makes it promising for enhancing AD imaging across multiple modalities. Brain metabolic region and structural region related to AD, CN, and MCI were localized on individual fused images. The related regions of individuals were partly similar to each other. As shown in [Fig. 6](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/), [Fig. 7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/), and [Fig. 8](https://pmc.ncbi.nlm.nih.gov/articles/PMC11320145/). This similarity implies indicates that there are similar regions of interest across the different classes. The combination of MRI and PET data, in particular, can provide the following advantages: * • Magnetic resonance imaging records structural brain integrity, while PET indicates functional alterations related to glucose metabolism or neuronal activity. * **–** The fusion of both of these modalities can build a better association between anatomical brain abnormalities and functional impairments by merging these complementary data, potentially offering a more thorough understanding of the neurodegenerative processes of AD. * **–** The fusion-based approach may provide extra discriminatory information for more accurate differential discrimination and reducing the rate of misdiagnoses. * • The integration of combined MRI and PET data holds the potential to uncover neuroanatomical biomarkers associated with Alzheimer's disease, offering a promising avenue for advancing our understanding of the diagnostic capabilities of computer vision-based approaches.\n",
      "\n",
      "## Conclusion\n",
      "The study presented an algorithm for classifying different stages of AD using fused images obtained from sMRI and FDG-PET imaging. The results showed promise, showing a good level of accuracy, sensitivity, and specificity in distinguishing Alzheimer's disease dementia from normal cognitive impairment, Alzheimer's disease dementia from mild cognitive impairment, and normal cognitive from mild cognitive impairment. This innovative approach, which uses a Pareto-optimized MViT model with GLP and SPCNN for image fusion, could overcome the limitations of error-prone feature selection techniques in multimodal imaging. By applying vision-transformer models for image categorization and fusion, more accurate and reliable AD analysis might be achievable, providing complementary data from multiple imaging modalities. The results of the OASIS and ADNI datasets suggest that this method could aid in early diagnosis, potentially improving patient outcomes and quality of life. With further validation and refinement, this technique could become a valuable tool for early identification of AD. Apromising avenue for the future research lies in enhancing the discriminatory capability of our diagnostic method to distinguish between amnestic and atypical presentations of AD. These distinct presentations may entail different patterns of brain involvement, and further optimization of our technique could yield insights into the specific brain areas affected in these varied AD subtypes. Such refinements hold the potential to contribute to more personalized approaches to diagnosis and treatment. Another intriguing direction for future exploration involves investigating the diagnostic value of our novel technique in distinguishing AD from other neurodegenerative disorders, such as frontotemporal lobar degenerations, Lewy body disorders, tauopathies, TDP-43 proteinopathies, and synucleinopathies. Given the potential overlap in affected brain regions among these conditions, our method could play a crucial role in providing clarity during the challenging process of making differential diagnoses, having the potential to significantly enhance our ability to differentiate AD from a spectrum of neurodegenerative disorders, thereby facilitating more accurate and timely clinical decisions. Finally, the integration of additional imaging modalities or clinical data could help to further enhance the precision and reliability of the approach, as would the further analysis of different high-pass filters, which could provide a more comprehensive understanding of the various factors that influence multimodal fusion in medical imaging analysis.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Find abstract, discussion and conclusion'''\n",
    "\n",
    "extracted_value_content = {\n",
    "  \"abstract\": \"\",\n",
    "  \"discussion\": \"\",\n",
    "  \"conclusion\": \"\"\n",
    "}\n",
    "\n",
    "demonstrate_markdown_content = \"\"\n",
    "\n",
    "# iterate through extracted_content\n",
    "for key, value in extracted_content.items():\n",
    "\n",
    "  # if key contains abstract, pull out\n",
    "  if any(word in key.lower() for word in [\"abstract\"]):\n",
    "    # extracted_value_content[\"abstract\"] = f\"{value}\"\n",
    "    demonstrate_markdown_content += f\"## Abstract\\n{value}\\n\\n\"\n",
    "\n",
    "  # if key contains discussion, pull out\n",
    "  if any(word in key.lower() for word in [\"discussion\"]):\n",
    "    # extracted_value_content[\"discussion\"] = f\"{value}\"\n",
    "    demonstrate_markdown_content += f\"## Discussion\\n{value}\\n\\n\"\n",
    "  # if key contains conclusion, pull out\n",
    "  if any(word in key.lower() for word in [\"conclusion\"]):\n",
    "    # extracted_value_content[\"conclusion\"] = f\"{value}\"\n",
    "    demonstrate_markdown_content += f\"## Conclusion\\n{value}\\n\\n\"\n",
    "\n",
    "print(demonstrate_markdown_content)\n",
    "\n",
    "\n",
    "\n",
    "# output as markdown file\n",
    "with open(\"../../data/raw_content/demonstrate_markdown_content.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(demonstrate_markdown_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
