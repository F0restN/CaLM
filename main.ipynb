{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query_text = 'how should I take care of my parents with Alzheimer disease?' # failed, no results\n",
    "user_id = 'John Snow'\n",
    "\n",
    "GENERATION_PROMPT = \"\"\"\n",
    "Based on the provided context, answer the user's question. If the context doesn't contain enough information or your not sure, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Return a clear and concise answer. The output should be in the following format:\n",
    "\n",
    "Answer: <answer>\n",
    "Sources: <sources>\n",
    "\n",
    "If context isn't provided, use \"Answer\" should start with \"Sorry, I don't have enough information to answer that question.\" and \"Sources\" should be empty.\n",
    "\"\"\"\n",
    "\n",
    "memories = mem0.search(query_text, user_id)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=GENERATION_PROMPT\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=query_text),\n",
    "    # (\"system\", prompt)\n",
    "    \n",
    "])\n",
    "\n",
    "# query_text = 'what is the specialty for Chinese language. ' \n",
    "\n",
    "local_llm = 'marco-o1'\n",
    "local_embedding_model = 'BAAI/bge-m3'\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0.0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0.0, format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vector store and embedding model\n",
    "\n",
    "Get peer support knowledge base and research knowledge base\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding.vector_store import VectorStore\n",
    "from embedding.embedding_models import EmbeddingModels\n",
    "\n",
    "embedding_model = EmbeddingModels().get_bge_embedding(model_name=local_embedding_model)\n",
    "vectorstore = VectorStore()\n",
    "\n",
    "peer_kb = vectorstore.get_chroma_vectorstore(\n",
    "    vectorstore_path='./data/vector_database/peer_kb', \n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "research_kb = vectorstore.get_chroma_vectorstore(\n",
    "    vectorstore_path='./data/vector_database/research_kb', \n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcheckpoints\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroutering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_routing_decision\n\u001b[0;32m----> 3\u001b[0m intention_detection \u001b[38;5;241m=\u001b[39m \u001b[43mget_routing_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_llm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development-Projects/LangchainRag_Ollama/src/checkpoints/routering.py:44\u001b[0m, in \u001b[0;36mget_routing_decision\u001b[0;34m(messages, model, temperature)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessages list cannot be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m last_message \u001b[38;5;241m=\u001b[39m messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mlast_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     45\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid message content type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(last_message\u001b[38;5;241m.\u001b[39mcontent)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage content must be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "from checkpoints.routering import get_routing_decision\n",
    "\n",
    "intention_detection = get_routing_decision(query_text, model=local_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "similarity_search_res = vectorstore.retrieve_docs(query=query_text, vectorstore=, k=5)\n",
    "\n",
    "for i, doc in enumerate(similarity_search_res):\n",
    "    print(f\"---------------------------||Document Number: {i} ||------------------------ \")\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve grading \n",
    "*Preventing hallucination and erroneous retrieval that is not relevant to the question but based on the idiosyncrasies of the embedding model or chunking.*\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from checkpoints.retrieval_grading import grade_retrieval\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Grade the retrieval results\n",
    "graded_docs = grade_retrieval(query_text, similarity_search_res, model=\"llama3.2\")\n",
    "\n",
    "# Print graded results\n",
    "# for doc in graded_docs:\n",
    "#     print(f\"Document content: {doc}\")\n",
    "    \n",
    "# Filter out documents with relevance_score lower than 0.5\n",
    "class ReasonedDocument:\n",
    "    \"\"\"\n",
    "    A class to store the document, relevance score, and reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    document: Document # Retrieved document from vector store in Document type\n",
    "    relevance_score: float # Relevance score by reasoning model\n",
    "    reasoning: str # Reasons for the relevance score\n",
    "    \n",
    "    def __init__(self, document, relevance_score, reasoning):\n",
    "        self.document = document\n",
    "        self.relevance_score = relevance_score\n",
    "        self.reasoning = reasoning\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"Document: {self.document}\\nRelevance Score: {self.relevance_score}\\nReasoning: {self.reasoning}\\n\"\n",
    "\n",
    "# filtered_docs = [doc for doc, grade in zip(similarity_search_res, graded_docs) if grade[\"relevance_score\"] >= 0.5]\n",
    "\n",
    "filtered_docs = []\n",
    "\n",
    "# Filter out documents with relevance_score lower than 0.5 and re-rank relevant documents based on relevance_score descending\n",
    "filtered_docs = []\n",
    "\n",
    "for doc, grade in zip(similarity_search_res, graded_docs):\n",
    "    if grade[\"relevance_score\"] >= 0.5:\n",
    "        reasoned_doc = ReasonedDocument(\n",
    "            document=doc,\n",
    "            relevance_score=grade[\"relevance_score\"],\n",
    "            reasoning=grade[\"reasoning\"]\n",
    "        )\n",
    "        filtered_docs.append(reasoned_doc)\n",
    "\n",
    "# Re-rank the filtered documents based on relevance_score in descending order\n",
    "filtered_docs.sort(key=lambda x: x.relevance_score, reverse=True)\n",
    "\n",
    "# Print filtered results\n",
    "for doc in filtered_docs:\n",
    "    print(doc)\n",
    "    # print(doc.metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answer\n",
    "\n",
    "*Using the graded documents to generate an answer.*\n",
    "\n",
    "1. Only use the graded documents to generate an answer.\n",
    "\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from answer_generation import generate_answer\n",
    "\n",
    "context_chunks = [doc.document for doc in filtered_docs]\n",
    "\n",
    "# TODO: handle the case where context_chunks is empty, answer should specify that we don't have relevant information.\n",
    "\n",
    "answer = generate_answer(query_text, context_chunks, model=\"marco-o1\")\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
